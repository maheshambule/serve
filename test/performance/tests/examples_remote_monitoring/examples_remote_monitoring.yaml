
~execution:
- concurrency: 4
  ramp-up: 1s
  hold-for: 20s
  scenario: Inference

~scenarios:
  Inference:
    script: examples_remote_monitoring.jmx



~services:
  - module: shellexec
    prepare:
      - "curl -s -O https://s3.amazonaws.com/model-server/inputs/kitten.jpg"
      - "mkdir /tmp/model_server"
      - "torchserve --start --model-store /tmp/model_server > /dev/null 2>&1"
      - "sleep 20s"
    post-process:
      - "torchserve --stop > /dev/null 2>&1"
      - "rm kitten.jpg"
      - "rm -r /tmp/model_server"
  - module: monitoring
    server-agent:
      - address: localhost:9009 # metric monitoring service address
        label: model-server  # if you specify label, it will be used in reports instead of ip:port
        interval: 1s    # polling interval
        logging: True # those logs will be saved to "SAlogs_192.168.0.1_9009.csv" in the artifacts dir
        metrics: # metrics should be supported by monitoring service
          - sum_all_cpu_percent # cpu percent used by all the Model server processes and workers
          - sum_workers_memory_percent
          - frontend_file_descriptors
          - total_workers # no of Model Server workers

~reporting:
  - module: passfail

~compare_criteria:
